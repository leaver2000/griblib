{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import subprocess\n",
    "\n",
    "# Get hourly files from June 1 and June 2 2021\n",
    "now = datetime.datetime(2021, 6, 1, 0)\n",
    "utc_end = datetime.datetime(2021, 6, 2, 0)\n",
    "\n",
    "while now < utc_end:\n",
    "    cmd = (\n",
    "        'wget -L -O ' + now.strftime('%Y%m%d%H') + '.zip ' +\n",
    "        'https://mrms.agron.iastate.edu/' + now.strftime('%Y/%m/%d/%Y%m%d%H') + '.zip'\n",
    "    )\n",
    "    print(cmd)\n",
    "    subprocess.run(cmd, shell=True)\n",
    "    now += datetime.timedelta(hours=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import subprocess\n",
    "\n",
    "url = \"https://mrms.agron.iastate.edu/2022/08/23/2022082300.zip\"\n",
    "TMP_ROOT = Path(\"/tmp/mmmpy/\")\n",
    "TMP_DIR = \"/tmp/mmmpy-tmp/\"\n",
    "\n",
    "class IAState:\n",
    "    def __init__(self, targetdir=TMP_ROOT) -> None:\n",
    "        self.targetdir = targetdir\n",
    "        self.tmpdir = Path(TMP_DIR)\n",
    "\n",
    "    def __enter__(self):\n",
    "\n",
    "        if not self.tmpdir.exists():\n",
    "            self.tmpdir.mkdir(parents=True)\n",
    "\n",
    "        if not self.targetdir.exists():\n",
    "            self.targetdir.mkdir(parents=True)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        if self.tmpdir.exists():\n",
    "            ...\n",
    "            # shutil.rmtree(self.tmpdir)\n",
    "\n",
    "    def get(self, url: str, raise_for_status=True):\n",
    "        _, filename = url.rsplit(\"/\", maxsplit=1)\n",
    "        outfile = self.tmpdir / filename#.removesuffix(\".zip\")\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with outfile.open(\"wb\") as f:\n",
    "                # shutil.copyfileobj(r.raw, fdst)\n",
    "                for chunk in r.iter_content(chunk_size=1024): \n",
    "                    if chunk: # filter out keep-alive new chunks\n",
    "                        f.write(chunk)\n",
    "        # return self.__unzip(outfile)\n",
    "\n",
    "    def __unzip(self, filename: str):\n",
    "        \"\"\"\n",
    "        -FF\n",
    "        --fixfix\n",
    "            Fix the zip archive. The -F option can be used if some\n",
    "            portions of the archive are missing, but requires a reasonably\n",
    "            intact central directory.   The  input  archive is scanned as\n",
    "            usual, but zip will ignore some problems.  The resulting\n",
    "            archive should be valid, but any inconsistent entries will be\n",
    "            left out.\n",
    "\n",
    "            When doubled as in -FF, the archive is scanned from the\n",
    "            beginning and zip scans  for  special  signatures  to\n",
    "            identify  the  limits between the archive members. The single\n",
    "            -F is more reliable if the archive is not too much damaged, so\n",
    "            try this option first.\n",
    "\n",
    "            If  the archive is too damaged or the end has been truncated,\n",
    "            you must use -FF.  This is a change from zip 2.32, where the\n",
    "            -F option is able to read a truncated archive.  The -F option\n",
    "            now more reliably fixes archives with minor damage and the -FF\n",
    "            option is  needed to fix archives where -F might have been\n",
    "            sufficient before.\n",
    "            ...\n",
    "        \"\"\"\n",
    "\n",
    "        with subprocess.Popen(\n",
    "            [\n",
    "                \"zip\",\n",
    "                \"-FF\",\n",
    "                TMP_DIR + filename,\n",
    "                # (self.tmpdir / filename).as_posix(),\n",
    "                \"--out\",\n",
    "                filename\n",
    "            ],\n",
    "        ):\n",
    "            ...\n",
    "\n",
    "\n",
    "# cmd = f\"zip -FFv /tmp/mmmpy-de1beace-234e-11ed-b5a2-0242ac110002/2022082300.zip --out /tmp/mmmpy/2022082300.zip\"\n",
    "# subprocess.run(cmd, shell=True)\n",
    "# with IAState() as s:\n",
    "#     s.get(url)\n",
    "# # filename= \"2022082300.zip\"\n",
    "# # with subprocess.Popen(\n",
    "# #     [\n",
    "# #         \"zip\",\n",
    "# #         \"-q\",\n",
    "# #         \"-FF\",\n",
    "# #         TMP_DIR+filename,\n",
    "# #         # \"/tmp/mmmpy-tmp/2022082300.zip\",\n",
    "# #         \"--out\",\n",
    "# #         filename,\n",
    "# #     ]\n",
    "# # ):  ...\n",
    "\n",
    "# # assert False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_PRODS = [\n",
    "    \"ANC_FinalForecast\",\n",
    "    \"ANC_ConvectiveLikelihood\",\n",
    "    \"EchoTop_60\",\n",
    "    \"RotationTrack240min\",\n",
    "    \"MultiSensor_QPE_03H_Pass2\",\n",
    "    \"RadarOnly_QPE_15M\",\n",
    "    \"MultiSensor_QPE_48H_Pass1\",\n",
    "    \"MultiSensor_QPE_01H_Pass2\",\n",
    "    \"RadarOnly_QPE_01H\",\n",
    "    \"RadarAccumulationQualityIndex_06H\",\n",
    "    \"MultiSensor_QPE_06H_Pass2\",\n",
    "    \"FLASH_SAC_MAXSOILSAT\",\n",
    "    \"H60_Above_-20C\",\n",
    "    \"Reflectivity_0C\",\n",
    "    \"PrecipFlag\",\n",
    "    \"EchoTop_18\",\n",
    "    \"RotationTrack1440min\",\n",
    "    \"MergedReflectivityQComposite\",\n",
    "    \"MultiSensor_QPE_24H_Pass2\",\n",
    "    \"MergedBaseReflectivityQC\",\n",
    "    \"GaugeInflIndex_72H_Pass1\",\n",
    "    \"RotationTrackML30min\",\n",
    "    \"Model_SurfaceTemp\",\n",
    "    \"MultiSensor_QPE_03H_Pass1\",\n",
    "    \"RadarAccumulationQualityIndex_03H\",\n",
    "    \"FLASH_CREST_MAXSOILSAT\",\n",
    "    \"BREF_1HR_MAX\",\n",
    "    \"FLASH_QPE_ARI12H\",\n",
    "    \"EchoTop_50\",\n",
    "    \"VIL_Max_120min\",\n",
    "    \"LayerCompositeReflectivity_High\",\n",
    "    \"Reflectivity_-15C\",\n",
    "    \"FLASH_QPE_ARI24H\",\n",
    "    \"RadarAccumulationQualityIndex_72H\",\n",
    "    \"FLASH_HP_MAXUNITSTREAMFLOW\",\n",
    "    \"NLDN_CG_015min_AvgDensity\",\n",
    "    \"MESH\",\n",
    "    \"RadarOnly_QPE_06H\",\n",
    "    \"SHI\",\n",
    "    \"GaugeInflIndex_01H_Pass2\",\n",
    "    \"Reflectivity_-10C\",\n",
    "    \"LVL3_HREET\",\n",
    "    \"LVL3_HighResVIL\",\n",
    "    \"LayerCompositeReflectivity_Super\",\n",
    "    \"GaugeInflIndex_24H_Pass2\",\n",
    "    \"CREF_1HR_MAX\",\n",
    "    \"H50_Above_0C\",\n",
    "    \"MultiSensor_QPE_48H_Pass2\",\n",
    "    \"GaugeInflIndex_03H_Pass2\",\n",
    "    \"LowLevelCompositeReflectivity\",\n",
    "    \"RadarAccumulationQualityIndex_48H\",\n",
    "    \"RotationTrackML60min\",\n",
    "    \"RadarAccumulationQualityIndex_24H\",\n",
    "    \"FLASH_SAC_MAXUNITSTREAMFLOW\",\n",
    "    \"MultiSensor_QPE_01H_Pass1\",\n",
    "    \"SeamlessHSRHeight\",\n",
    "    \"WarmRainProbability\",\n",
    "    \"RotationTrack60min\",\n",
    "    \"FLASH_CREST_MAXSTREAMFLOW\",\n",
    "    \"Reflectivity_-5C\",\n",
    "    \"GaugeInflIndex_24H_Pass1\",\n",
    "    \"RotationTrackML360min\",\n",
    "    \"FLASH_QPE_FFG03H\",\n",
    "    \"HeightCompositeReflectivity\",\n",
    "    \"MultiSensor_QPE_72H_Pass1\",\n",
    "    \"GaugeInflIndex_12H_Pass1\",\n",
    "    \"FLASH_QPE_ARI30M\",\n",
    "    \"FLASH_QPE_ARIMAX\",\n",
    "    \"MultiSensor_QPE_12H_Pass1\",\n",
    "    \"VII\",\n",
    "    \"MergedZdr\",\n",
    "    \"MergedReflectivityQC\",\n",
    "    \"MESH_Max_240min\",\n",
    "    \"GaugeInflIndex_01H_Pass1\",\n",
    "    \"Model_0degC_Height\",\n",
    "    \"FLASH_QPE_ARI06H\",\n",
    "    \"RadarOnly_QPE_12H\",\n",
    "    \"LayerCompositeReflectivity_Low\",\n",
    "    \"FLASH_QPE_ARI01H\",\n",
    "    \"MESH_Max_60min\",\n",
    "    \"VIL\",\n",
    "    \"MESH_Max_360min\",\n",
    "    \"RadarAccumulationQualityIndex_01H\",\n",
    "    \"MergedAzShear_0-2kmAGL\",\n",
    "    \"MergedReflectivityComposite\",\n",
    "    \"FLASH_QPE_FFG06H\",\n",
    "    \"PrecipRate\",\n",
    "    \"NLDN_CG_030min_AvgDensity\",\n",
    "    \"Model_WetBulbTemp\",\n",
    "    \"POSH\",\n",
    "    \"RadarOnly_QPE_24H\",\n",
    "    \"SeamlessHSR\",\n",
    "    \"VIL_Max_1440min\",\n",
    "    \"MESH_Max_30min\",\n",
    "    \"RadarAccumulationQualityIndex_12H\",\n",
    "    \"NLDN_CG_001min_AvgDensity\",\n",
    "    \"GaugeInflIndex_12H_Pass2\",\n",
    "    \"GaugeInflIndex_48H_Pass2\",\n",
    "    \"FLASH_SAC_MAXSTREAMFLOW\",\n",
    "    \"NLDN_CG_005min_AvgDensity\",\n",
    "    \"MergedReflectivityQCComposite\",\n",
    "    \"RotationTrack360min\",\n",
    "    \"RadarOnly_QPE_48H\",\n",
    "    \"ReflectivityAtLowestAltitude\",\n",
    "    \"RadarOnly_QPE_03H\",\n",
    "    \"MultiSensor_QPE_06H_Pass1\",\n",
    "    \"BrightBandBottomHeight\",\n",
    "    \"H60_Above_0C\",\n",
    "    \"H50_Above_-20C\",\n",
    "    \"EchoTop_30\",\n",
    "    \"GaugeInflIndex_48H_Pass1\",\n",
    "    \"FLASH_QPE_FFG01H\",\n",
    "    \"GaugeInflIndex_06H_Pass1\",\n",
    "    \"LayerCompositeReflectivity_ANC\",\n",
    "    \"FLASH_QPE_ARI03H\",\n",
    "    \"Reflectivity_-20C\",\n",
    "    \"MergedRhoHV\",\n",
    "    \"BrightBandTopHeight\",\n",
    "    \"MESH_Max_1440min\",\n",
    "    \"RadarQualityIndex\",\n",
    "    \"MergedAzShear_3-6kmAGL\",\n",
    "    \"RotationTrackML1440min\",\n",
    "    \"RotationTrack120min\",\n",
    "    \"HeightLowLevelCompositeReflectivity\",\n",
    "    \"GaugeInflIndex_72H_Pass2\",\n",
    "    \"MergedBaseReflectivity\",\n",
    "    \"RadarOnly_QPE_Since12Z\",\n",
    "    \"MultiSensor_QPE_72H_Pass2\",\n",
    "    \"FLASH_HP_MAXSTREAMFLOW\",\n",
    "    \"FLASH_CREST_MAXUNITSTREAMFLOW\",\n",
    "    \"MESH_Max_120min\",\n",
    "    \"RotationTrackML240min\",\n",
    "    \"MultiSensor_QPE_24H_Pass1\",\n",
    "    \"RotationTrack30min\",\n",
    "    \"MultiSensor_QPE_12H_Pass2\",\n",
    "    \"GaugeInflIndex_06H_Pass2\",\n",
    "    \"SyntheticPrecipRateID\",\n",
    "    \"MergedReflectivityAtLowestAltitude\",\n",
    "    \"FLASH_QPE_FFGMAX\",\n",
    "    \"RadarOnly_QPE_72H\",\n",
    "    \"VIL_Density\",\n",
    "    \"GaugeInflIndex_03H_Pass1\",\n",
    "    \"RotationTrackML120min\",\n",
    "]\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class MyEnum:\n",
    "    def __getitem__(self, __o):\n",
    "        print(__o)\n",
    "\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "def string_enum(func: \"El\"):\n",
    "    for member in dir(func):\n",
    "        if not member.startswith(\"__\"):\n",
    "            print(member)\n",
    "\n",
    "    def inner():\n",
    "        return func()\n",
    "\n",
    "    return inner\n",
    "\n",
    "\n",
    "class El(str, Enum):\n",
    "    H60_Above = \"H60_Above_-20C\"\n",
    "    H61_Above = \"H60_Abov3e_-20C\"\n",
    "\n",
    "\n",
    "class E:\n",
    "    def __new__(cls, e: Enum):\n",
    "        cls.__e = e\n",
    "        return cls\n",
    "\n",
    "    @classmethod\n",
    "    def loc(cls, items: list[str] = None):\n",
    "        if items:\n",
    "            return [cls.__e[item].value for item in items]\n",
    "        return cls.__e\n",
    "\n",
    "\n",
    "e = E(El)\n",
    "e.loc([\"H60_Above\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "tuple((Path.cwd().parent / \"data\").glob(\"*.zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "import gzip\n",
    "import subprocess\n",
    "import zipfile\n",
    "import shutil\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Literal, Iterable, NewType \n",
    "\n",
    "FileName = NewType(\"FileName\",str)\n",
    "ALL_REGIONS = [\"ANC\", \"CONUS\", \"ALASKA\", \"CARIB\", \"GUAM\", \"HAWAII\"]\n",
    "MRMSRegion=list[Literal[\"ANC\", \"CONUS\", \"ALASKA\", \"CARIB\", \"GUAM\", \"HAWAII\"]]\n",
    "MRMSProduct=list[Literal[]]\n",
    "class IAStateZip(zipfile.ZipFile):\n",
    "    def filterinfo(\n",
    "        self,\n",
    "        regions: MRMSRegion,\n",
    "        products: list[str],\n",
    "    ) -> Iterable[tuple[zipfile.ZipInfo, FileName]]:\n",
    "        df = pd.DataFrame({\"zipInfo\": self.infolist(), \"path\": self.namelist()})\n",
    "        df = df[df[\"path\"].str.endswith(\".gz\")]\n",
    "        df.loc[:, [\"validTime\", \"region\", \"product\", \"name\"]] = np.vstack(\n",
    "            df[\"path\"].str.split(\"/\")\n",
    "        )\n",
    "        region_mask = np.any(\n",
    "            (np.array(regions)[:, np.newaxis] == df[\"region\"].values).T, axis=1\n",
    "        )\n",
    "        product_mask = np.any(\n",
    "            (np.array(products)[:, np.newaxis] == df[\"product\"].values).T, axis=1\n",
    "        )\n",
    "        yield from df.loc[(region_mask & product_mask), [\"zipInfo\", \"name\"]].to_numpy()\n",
    "\n",
    "\n",
    "def iastate_connect(start: datetime, out: Path, fix_zip: bool = True) -> Path:\n",
    "    url = start.strftime(\"https://mrms.agron.iastate.edu/%Y/%m/%d/%Y%m%d%H.zip\")\n",
    "    print(\"downloading mrms data from:\", url)\n",
    "    _, filename = url.rsplit(\"/\", maxsplit=1)\n",
    "    file = out / filename\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with file.open(\"wb\") as fd:\n",
    "            for chunk in r.iter_content(chunk_size=4096):\n",
    "                fd.write(chunk)\n",
    "    if fix_zip:\n",
    "        replace_badzip(file)\n",
    "    return file\n",
    "\n",
    "\n",
    "def replace_badzip(corrupt: Path, in_place:bool=True):\n",
    "    print(\"attempting to resolve fix zipfile\")\n",
    "\n",
    "    tmpfile = corrupt.parent / f\"{uuid.uuid1()}.zip\"\n",
    "\n",
    "    subprocess.call(\n",
    "        [\"zip\", \"-FF\", corrupt.as_posix(), f\"--out={tmpfile.as_posix()}\"],\n",
    "        stdout=subprocess.DEVNULL,\n",
    "    )\n",
    "    if in_place:\n",
    "        corrupt.unlink()\n",
    "        shutil.move(tmpfile, corrupt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__1\":\n",
    "    data = Path.cwd().parent / \"data\"\n",
    "    zfile = iastate_connect(\n",
    "        datetime.fromisoformat(\"2022-06-01T12\"), out=data\n",
    "    )\n",
    "    assert zfile.is_file()\n",
    "    # open the zip file\n",
    "    with IAStateZip(zfile) as zf:\n",
    "        # filter the info inside of the zip\n",
    "        for member, filename in zf.filterinfo(regions=[\"CONUS\"], products=[\"MergedReflectivityQC\"]):\n",
    "            # create a new filename\n",
    "            file = zfile.parent / filename.removesuffix(\".gz\")\n",
    "            # read the member from the zip file\n",
    "            with zf.open(member,mode=\"r\") as zref:\n",
    "                # open a new file\n",
    "                with file.open(\"wb\") as fdst:\n",
    "                    # wrote the unziped & gunziped file to the new folder\n",
    "                    fdst.write(gzip.decompress(zref.read()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Literal, Iterable, NewType\n",
    "\n",
    "FileName = NewType(\"FileName\", str)\n",
    "ALL_REGIONS = [\"ANC\", \"CONUS\", \"ALASKA\", \"CARIB\", \"GUAM\", \"HAWAII\"]\n",
    "data = Path.cwd().parent / \"data/2022060112.zip\"\n",
    "\n",
    "\n",
    "def iterzipinfo(\n",
    "    zf: zipfile.ZipFile,\n",
    "    regions: list[Literal[\"ANC\", \"CONUS\", \"ALASKA\", \"CARIB\", \"GUAM\", \"HAWAII\"]],\n",
    "    products: list[str],\n",
    ") -> Iterable[zipfile.ZipInfo]:\n",
    "    df = pd.DataFrame({\"zipInfo\": zf.infolist(), \"path\": zf.namelist()})\n",
    "    df = df[df[\"path\"].str.endswith(\".gz\")]\n",
    "    df.loc[:, [\"validTime\", \"region\", \"product\", \"name\"]] = np.vstack(\n",
    "        df[\"path\"].str.split(\"/\")\n",
    "    )\n",
    "    region_mask = np.any(\n",
    "        (np.array(regions)[:, np.newaxis] == df[\"region\"].values).T, axis=1\n",
    "    )\n",
    "    product_mask = np.any(\n",
    "        (np.array(products)[:, np.newaxis] == df[\"product\"].values).T, axis=1\n",
    "    )\n",
    "    yield from df.loc[(region_mask & product_mask), \"zipInfo\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PandasZip(zipfile.ZipFile):\n",
    "    def filterinfo(\n",
    "        self,\n",
    "        regions: list[Literal[\"ANC\", \"CONUS\", \"ALASKA\", \"CARIB\", \"GUAM\", \"HAWAII\"]],\n",
    "        products: list[str],\n",
    "    ) -> Iterable[tuple[zipfile.ZipInfo, FileName]]:\n",
    "        df = pd.DataFrame({\"zipInfo\": self.infolist(), \"path\": self.namelist()})\n",
    "        df = df[df[\"path\"].str.endswith(\".gz\")]\n",
    "        df.loc[:, [\"validTime\", \"region\", \"product\", \"name\"]] = np.vstack(\n",
    "            df[\"path\"].str.split(\"/\")\n",
    "        )\n",
    "        region_mask = np.any(\n",
    "            (np.array(regions)[:, np.newaxis] == df[\"region\"].values).T, axis=1\n",
    "        )\n",
    "        product_mask = np.any(\n",
    "            (np.array(products)[:, np.newaxis] == df[\"product\"].values).T, axis=1\n",
    "        )\n",
    "        yield from df.loc[(region_mask & product_mask), [\"zipInfo\", \"name\"]].to_numpy()\n",
    "\n",
    "\n",
    "def zip_locate_extract(\n",
    "    zippath: Path,\n",
    "    regions: list[Literal[\"ANC\", \"CONUS\", \"ALASKA\", \"CARIB\", \"GUAM\", \"HAWAII\"]],\n",
    "    products: list[str],\n",
    "):\n",
    "    assert zippath.suffix == \".zip\"\n",
    "\n",
    "    with PandasZip(zippath) as pz:\n",
    "        for member, filename in pz.filterinfo(regions, products):\n",
    "            with pz.open(member, \"r\") as ref:\n",
    "                with (zippath.parent / filename.removesuffix(\".gz\")).open(\"wb\") as fdst:\n",
    "                    fdst.write(gzip.decompress(ref.read()))\n",
    "\n",
    "\n",
    "zip_locate_extract(data, regions=[\"CONUS\"], products=[\"MergedReflectivityQC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = Path.cwd().parent / \"data\"\n",
    "corrupt = data / \"2022072523.zip\"\n",
    "\n",
    "\n",
    "def replace_badzip(corrupt: Path, tmpdir: Path = Path(\"/tmp/mmmpy\")):\n",
    "    print(\"attempting to resolve fix zipfile\")\n",
    "    if not tmpdir.exists():\n",
    "        tmpdir.mkdir()\n",
    "    tmpfile = tmpdir / f\"{uuid.uuid1()}.zip\"\n",
    "    subprocess.call(\n",
    "        [\"zip\", \"-FF\", corrupt.as_posix(), f\"--out={tmpfile.as_posix()}\"],\n",
    "        stdout=subprocess.DEVNULL,\n",
    "    )\n",
    "\n",
    "    corrupt.unlink()\n",
    "    shutil.move(tmpfile, corrupt)\n",
    "\n",
    "\n",
    "replace_badzip(corrupt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import gzip\n",
    "\n",
    "WRITE_BINARY = \"wb\"\n",
    "READ_BINARY = \"rb\"\n",
    "GZ = \".gz\"\n",
    "ZIP = \".zip\"\n",
    "\n",
    "\n",
    "def unzip_decompress(\n",
    "    file: Path, out: Path = Path.cwd(), region=\"CONUS\", product=\"MergedReflectivityQC\"\n",
    "):\n",
    "    # only valid zip files\n",
    "    assert file.is_file() and ZIP in file.suffixes\n",
    "    # make the directory to dump the data\n",
    "    if not out.exists():\n",
    "        out.mkdir()\n",
    "    # open the zipfuke\n",
    "    with zipfile.ZipFile(file, mode=\"r\") as zf:\n",
    "        # loop through the\n",
    "        for member in zf.infolist():\n",
    "            if (\n",
    "                member.filename.endswith(GZ)\n",
    "                and f\"{region}/{product}/\" in member.filename\n",
    "            ):\n",
    "                print(\"extracting:\\t\", member.filename)\n",
    "                with zf.open(member) as zref:\n",
    "                    filename = member.filename.rsplit(\"/\", maxsplit=1)[-1].removesuffix(\n",
    "                        GZ\n",
    "                    )\n",
    "                    with (out / filename).open(WRITE_BINARY) as fdst:\n",
    "                        fdst.write(gzip.decompress(zref.read()))\n",
    "\n",
    "    return tuple(out.rglob(\"*\"))\n",
    "\n",
    "\n",
    "files = unzip_decompress(corrupt, corrupt.parent / \"MergedReflectivityQC\")\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "from typing import Iterable\n",
    "import pandas as pd\n",
    "\n",
    "LEVEL_VALIDTIME = r\"(?P<level>\\d{2}\\.\\d{2})_(?P<validTime>.*).grib2\"\n",
    "\n",
    "def __valid_time(df:pd.DataFrame)->pd.DataFrame:\n",
    "    df[\"validTime\"] =  pd.to_datetime(df[\"validTime\"])\n",
    "    return df\n",
    "\n",
    "def __index(s:\"pd.Series[Path]\")->\"pd.Series[Path]\":\n",
    "    s.index = s.apply(lambda p:p.name).str.extract(LEVEL_VALIDTIME).pipe(__valid_time).pipe(pd.MultiIndex.from_frame)\n",
    "    return s.swaplevel().sort_index()\n",
    "\n",
    "def path_series(files:Iterable[Path])->\"pd.Series[Path]\":\n",
    "    return pd.Series(files, name=\"files\").pipe(__index)\n",
    "\n",
    "s = path_series(files)\n",
    "s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "from typing import Hashable, Iterable, Literal, Iterator\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "VALID_TIME = \"valid_time\"\n",
    "LEVEL_VALIDTIME = r\"(?P<level>\\d{2}\\.\\d{2})_(?P<validTime>.*).grib2\"\n",
    "\n",
    "\n",
    "class IAState2:\n",
    "    __vdims = {\"MergedReflectivityQC\": [\"heightAboveSea\"]}\n",
    "\n",
    "    def __init__(self, files: Iterable[Path]) -> None:\n",
    "        self.files = files\n",
    "\n",
    "    def extract(self):\n",
    "        ...\n",
    "\n",
    "    def pathseries(self) -> \"pd.Series[Path]\":\n",
    "        def __valid_time(df: pd.DataFrame) -> pd.DataFrame:\n",
    "            df[\"validTime\"] = pd.to_datetime(df[\"validTime\"])\n",
    "            return df\n",
    "\n",
    "        s = pd.Series(self.files, name=\"files\")\n",
    "        s.index = pd.MultiIndex.from_frame(\n",
    "            s.apply(lambda p: p.name).str.extract(LEVEL_VALIDTIME).pipe(__valid_time)\n",
    "        )\n",
    "        return s.swaplevel().sort_index()\n",
    "\n",
    "    def groupby_validtime(self) -> Iterator[tuple[datetime, \"pd.Series[Path]\"]]:\n",
    "        yield from self.pathseries().groupby(\"validTime\")\n",
    "\n",
    "    def open_mfdataset(\n",
    "        self,\n",
    "        files: Iterable[Path],\n",
    "        key=\"MergedReflectivityQC\",\n",
    "    ):\n",
    "        ds = xr.open_mfdataset(\n",
    "            files,\n",
    "            chunks={},\n",
    "            engine=\"cfgrib\",\n",
    "            combine=\"nested\",\n",
    "            concat_dim=self.__vdims.get(key),\n",
    "            backend_kwargs={\"time_dims\": [\"valid_time\"]},\n",
    "        ).rename({\"unknown\": key, \"valid_time\": \"validTime\"})\n",
    "        # fix variable attributes\n",
    "        ds[key].attrs = {k: v for k, v in ds[key].attrs.items() if v != \"unknown\"}\n",
    "        # clear dataset attributes\n",
    "        ds.attrs.clear()\n",
    "        return ds\n",
    "\n",
    "    def load(self):\n",
    "        ...\n",
    "\n",
    "\n",
    "ia = IAState2(files)\n",
    "for vt, path in tuple(ia.groupby_validtime())[:1]:\n",
    "\n",
    "    ds = ia.open_mfdataset(tuple(path)[:2])\n",
    "#     print(ds)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://svaderia.github.io/articles/downloading-and-unzipping-a-zipfile/\n",
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "url = \"https://mrms.agron.iastate.edu/2022/08/23/2022082300.zip\"\n",
    "_,filename = url.rsplit(\"/\",maxsplit=1)\n",
    "file = Path.cwd() / filename.removesuffix(\".zip\")\n",
    "with urlopen(url) as zipped_response:\n",
    "    with ZipFile(BytesIO(zipped_response.read())) as zfile:\n",
    "        zfile.extractall(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "# def unzip (path, total_count):\n",
    "#     for root, dirs, files in os.walk(path):\n",
    "#         for file in files:\n",
    "#             file_name = os.path.join(root, file)\n",
    "#             if (not file_name.endswith('.zip')):\n",
    "#                 total_count += 1\n",
    "#             else:\n",
    "#                 currentdir = file_name[:-4]\n",
    "#                 if not os.path.exists(currentdir):\n",
    "#                     os.makedirs(currentdir)\n",
    "#                 with ZipFile(file_name) as zipObj:\n",
    "#                     zipObj.extractall(currentdir)\n",
    "#                 os.remove(file_name)\n",
    "#                 total_count = unzip(currentdir, total_count)\n",
    "#     return total_count\n",
    "import gzip\n",
    "f = Path(\"/tmp/mmmpy/2022082300.zip\")\n",
    "\n",
    "import zipfile\n",
    "z = zipfile.ZipFile(\"/tmp/mmmpy/2022082300.zip\")\n",
    "# z.infolist()\n",
    "# z.extractall()\n",
    "f = [f.filename for f in z.infolist() if not f.filename.endswith(\".gz\")][0]\n",
    "z.extract(member=\"2022082300/ANC/ANC_FinalForecast/\")\n",
    "\n",
    "# z.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f= open(\"/tmp/mmmpy/2022082300.zip\")\n",
    "zipfile.is_zipfile(\"/tmp/mmmpy/2022082300.zip\")\n",
    "# gzip.\n",
    "with ZipFile(\"/tmp/mmmpy/2022082300.zip\") as myzip:\n",
    "    with myzip.open('eggs.txt') as myfile:\n",
    "       eggs = io.TextIOWrapper(myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r.content\n",
    "import io\n",
    "import zipfile\n",
    "\n",
    "with io.BytesIO(r.content) as buffer:\n",
    "    with zipfile.ZipFile(buffer) as z:\n",
    "        z.extractall(\"/tmp/mmmpy\")\n",
    "# from pathlib import Path\n",
    "\n",
    "# # import zipfile\n",
    "\n",
    "# import shutil\n",
    "\n",
    "# tmpdir = Path(\"/tmp/mmmpy\")\n",
    "# with (tmpdir / \"2022082300.zip\").open(\"wb\") as fdst:\n",
    "#     shutil.copyfileobj(r.raw, fdst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuple(tmpdir.glob(\"*\"))file\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(tmpdir / \"2022082300.zip\", ) as z:\n",
    "    print(z)\n",
    "\n",
    "# file = tmpdir / \"2022082300.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "\n",
    "FILE_NAME_PATTERN = re.compile(r\"/([A-Za-z]+(?:-|_)?[A-Za-z]+)+\")\n",
    "data = Path(os.path.abspath(__name__)).parents[1] / \"data\"\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted((data / \"MRMS_MergedReflectivity\").glob(\"*grib2\"))[:4]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dims(ds: xr.Dataset) -> xr.Dataset:\n",
    "    duplicates = [\"heightAboveSea\"]\n",
    "    # if more than one file was passed the valid_time should be greater than 1\n",
    "    if ds.valid_time.size > 1:\n",
    "        # for which we add a new validTime dimension\n",
    "        ds = ds.expand_dims({\"validTime\": ds[\"valid_time\"].to_numpy()})\n",
    "        duplicates.append(\"validTime\")\n",
    "\n",
    "    return ds.drop(\"valid_time\").drop_duplicates(duplicates)\n",
    "\n",
    "\n",
    "def name(ds: xr.Dataset) -> xr.Dataset:\n",
    "    if len(ds.data_vars) != 1:\n",
    "        # mrms grib2 data should only have one variable\n",
    "        raise Exception\n",
    "    (ds_name,) = ds\n",
    "    # not storing history, will use the history object to infer a name\n",
    "    hist = ds.attrs.pop(\"history\", None)\n",
    "    # if a name was not explicility provided\n",
    "    # if not name:\n",
    "    # use the known name if unknow infer one from the file name\n",
    "    if ds_name != \"unknown\":\n",
    "        name = ds_name\n",
    "    else:\n",
    "        name_list = FILE_NAME_PATTERN.findall(hist)\n",
    "        if name_list:\n",
    "            name = name_list[-1]\n",
    "        else:\n",
    "            raise Exception\n",
    "\n",
    "    return ds.rename({ds_name: name})\n",
    "\n",
    "\n",
    "def main():\n",
    "    ds = (\n",
    "        xr.open_mfdataset(\n",
    "            files,\n",
    "            chunks={},\n",
    "            engine=\"cfgrib\",\n",
    "            data_vars=\"minimal\",\n",
    "            combine=\"nested\",\n",
    "            concat_dim=[\"heightAboveSea\"],\n",
    "            backend_kwargs=dict(\n",
    "                mask_and_scale=True,\n",
    "                decode_times=True,\n",
    "                concat_characters=True,\n",
    "                decode_coords=True,\n",
    "                # use_cftime=\"%Y-%m\",\n",
    "                decode_timedelta=None,\n",
    "                lock=None,\n",
    "                indexpath=\"{path}.{short_hash}.idx\",\n",
    "                filter_by_keys={},\n",
    "                read_keys=[],\n",
    "                encode_cf=(\"parameter\", \"time\", \"geography\", \"vertical\"),\n",
    "                squeeze=True,\n",
    "                time_dims={\"valid_time\"},\n",
    "            ),\n",
    "        )\n",
    "        .pipe(dims)\n",
    "        .pipe(name)\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "\n",
    "# ds = main()\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfds = ds.copy()\n",
    "mfds.to_zarr(data / \"mfds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.open_dataset(data / \"mfds\", engine=\"zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate():\n",
    "    for file in files[:4]:\n",
    "        ds1 = xr.open_dataset(\n",
    "            file,\n",
    "            engine=\"cfgrib\",\n",
    "            chunks={},\n",
    "            # data_vars=\"minimal\",\n",
    "            # combine=\"nested\",\n",
    "            # concat_dim=[\"heightAboveSea\"],\n",
    "            backend_kwargs=dict(\n",
    "                mask_and_scale=True,\n",
    "                decode_times=True,\n",
    "                concat_characters=True,\n",
    "                decode_coords=True,\n",
    "                # use_cftime=\"%Y-%m\",\n",
    "                decode_timedelta=None,\n",
    "                lock=None,\n",
    "                indexpath=\"{path}.{short_hash}.idx\",\n",
    "                filter_by_keys={},\n",
    "                read_keys=[],\n",
    "                encode_cf=(\"parameter\", \"time\", \"geography\", \"vertical\"),\n",
    "                squeeze=True,\n",
    "                time_dims={\"valid_time\"},\n",
    "            ),\n",
    "        )\n",
    "        yield ds1.expand_dims(\n",
    "            {\n",
    "                \"validTime\": [ds1[\"valid_time\"].to_numpy()],\n",
    "                \"heightAboveSea\": [ds1[\"heightAboveSea\"].to_numpy()],\n",
    "            }\n",
    "        ).drop_duplicates([\"validTime\", \"heightAboveSea\"]).drop(\"valid_time\").pipe(\n",
    "            name\n",
    "        )\n",
    "\n",
    "\n",
    "datasets = tuple(generate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in xr.concat(datasets,dim=\"validTime\").drop_duplicates(\"validTime\").groupby(\"validTime\"):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import zarr\n",
    "\n",
    "teststore = data / \"test-bucket\"\n",
    "group_name = \"3DRefl\"\n",
    "\n",
    "if teststore.exists():\n",
    "    shutil.rmtree(teststore)\n",
    "import numpy as np\n",
    "for vt, ds in xr.concat(datasets,dim=\"validTime\").drop_duplicates(\"validTime\").groupby(\"validTime\"):\n",
    "    (dsname,) = ds\n",
    "    ds = ds.expand_dims({\"validTime\":[vt]}).fillna(np.nan)\n",
    "\n",
    "    ds[\"MRMS_MergedReflectivityQC\"].attrs.clear() \n",
    "    if not teststore.exists():\n",
    "        ds.to_zarr(\n",
    "            teststore,\n",
    "            mode=\"a\",\n",
    "            group=group_name,\n",
    "            compute=True,\n",
    "            \n",
    "        )\n",
    "    else:\n",
    "        ds.drop([\"latitude\",\"longitude\",\"heightAboveSea\"]).to_zarr(\n",
    "            teststore,\n",
    "            mode=\"a\",\n",
    "            group=group_name,\n",
    "            append_dim=\"validTime\",\n",
    "            compute=True,\n",
    "        )\n",
    "\n",
    "xr.open_zarr(teststore / group_name, consolidated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "paths = pd.Series((data / \"MRMS_MergedReflectivity\").glob(\"*.grib2\"))\n",
    "vt=pd.to_datetime(paths.apply(lambda p: p.name).str.extract(r\"(\\d{8}-\\d{6})\",expand=False))\n",
    "for vt,x in paths.groupby(vt):\n",
    "    print(tuple(x))\n",
    "    # print(x)\n",
    "# pd.to_datetime(for p in paths)#pd.Series(x.name for x in (data / \"MRMS_MergedReflectivity\").glob(\"*.grib2\")).str.extract(r\"(\\d{8}-\\d{6})\",expand=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(Group)\n",
    "import zarr\n",
    "root:zarr.Group = zarr.group(teststore / group)\n",
    "def unpack_root()->tuple[zarr.Array,...]:\n",
    "    yield from (root[key] for key in (\"latitude\",\"longitude\",\"heightAboveSea\",\"validTime\"))\n",
    "x,y,z,t = unpack_root()\n",
    "tuple(z)\n",
    "# x,y,z,t :tuple[zarr.Array,...]= root[\"latitude\"], root[\"longitude\"],  root[\"heightAboveSea\"], root[\"validTime\"],\n",
    "# x,y\n",
    "# z.values\n",
    "# g:zarr.Group = zarr.open(teststore / group)\n",
    "# tuple(g.array_keys())\n",
    "# g.info.obj\n",
    "# g.info_items()\n",
    "# g.store.items()\n",
    "# g.c\n",
    "# vt: np.datetime64 = ds[\"validTime\"].values[0]\n",
    "# (vt,) = pd.to_datetime(ds[\"validTime\"].values).strftime(\"%Y-%m-%dT\")\n",
    "\n",
    "# ds[\"validTime\"].to_numpy().astype(\"datetime64[m]\")\n",
    "# vt\n",
    "# np.str\n",
    "# import zarr\n",
    "\n",
    "# g: zarr.Group = zarr.open(teststore / \"3DRefl\")\n",
    "# cs: zarr.DirectoryStore = g.chunk_store\n",
    "# g.get(\"validTime\").size\n",
    "# # print(g.get(\"validTime\"))\n",
    "# # tuple(g)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "032fa08126f21d0740a292a80e9fc94cb2fda5367393b036481b5a8511343624"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0717c53d6b06231a47d82f28bf5c85ba985f82808dfef8e7f3be1cf4215fce4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
